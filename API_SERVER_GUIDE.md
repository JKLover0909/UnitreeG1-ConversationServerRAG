# QUICK START - API SERVER
# =====================================================

## ğŸš€ Khá»Ÿi Ä‘á»™ng nhanh trÃªn PC (Windows)

### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies

```powershell
# CÃ i Ä‘áº·t packages
pip install -r requirements_api_server.txt
```

### BÆ°á»›c 2: Set API Key

```powershell
# Temporary (chá»‰ trong terminal hiá»‡n táº¡i)
$env:OPENAI_API_KEY="sk-your-openai-key-here"

# Hoáº·c permanent (System Environment Variables)
# Win + R -> sysdm.cpl -> Advanced -> Environment Variables
# ThÃªm OPENAI_API_KEY = sk-your-key
```

### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng server

**Option A: Tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)**
```powershell
.\start_system.bat
```
- Tá»± Ä‘á»™ng má»Ÿ API server + NGrok
- Tá»± Ä‘á»™ng má»Ÿ Swagger docs táº¡i http://localhost:5000/docs

**Option B: Thá»§ cÃ´ng**
```powershell
# Terminal 1: Start API server
python api_server.py

# Terminal 2: Start NGrok
ngrok http 5000
```

### BÆ°á»›c 4: Test API

**Option 1: Swagger UI (Interactive)**
- Má»Ÿ browser: http://localhost:5000/docs
- Test tá»«ng endpoint trá»±c tiáº¿p trÃªn web

**Option 2: Test Script**
```powershell
# Test vá»›i file WAV cÃ³ sáºµn
python test_api_local.py your_audio.wav

# Hoáº·c test vá»›i WAV tá»± Ä‘á»™ng táº¡o
python test_api_local.py
```

**Option 3: cURL**
```powershell
# Health check
curl http://localhost:5000/health

# Process audio
curl -X POST -F "audio=@test.wav" http://localhost:5000/process_audio -o response.wav
```

---

## ğŸ“š KIáº¾N TRÃšC CODE

### Class Structure

```
core_openai.py (OOP Architecture)
â”œâ”€â”€ MeiRoboConfig        # Configuration dataclass
â”œâ”€â”€ RAGService          # Vector store & retrieval
â”œâ”€â”€ STTService          # Speech-to-Text
â”œâ”€â”€ LLMService          # OpenAI GPT + RAG
â”œâ”€â”€ TTSService          # Text-to-Speech
â””â”€â”€ MeiRoboPipeline     # Full pipeline orchestration

api_server.py (FastAPI)
â”œâ”€â”€ Uses MeiRoboPipeline
â”œâ”€â”€ RESTful endpoints
â””â”€â”€ Swagger docs auto-generated
```

### Key Differences from Old Version

**âœ… Advantages:**
1. **Class-based:** Clean OOP structure, easy to maintain
2. **FastAPI:** 
   - Automatic API docs (Swagger)
   - Type hints + validation
   - Better performance (async support)
   - Modern Python ecosystem
3. **Separation of concerns:** Each service is independent
4. **Reusable:** Can import classes for different use cases
5. **Short API code:** `api_server.py` is only ~150 lines

---

## ğŸ”§ CONFIGURATION

### Thay Ä‘á»•i cáº¥u hÃ¬nh

Edit trong [api_server.py](api_server.py) trÆ°á»›c khi start:

```python
# Táº¡o custom config
from core_openai import MeiRoboConfig, MeiRoboPipeline

config = MeiRoboConfig(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    llm_model="gpt-4o-mini",      # Hoáº·c "gpt-4o"
    tts_voice="nova",              # nova, alloy, echo, fable, onyx, shimmer
    tts_speed=1.2,                 # TÄƒng tá»‘c Ä‘á»™ nÃ³i
    rag_k=1,                       # Giáº£m docs retrieve (nhanh hÆ¡n)
    max_tokens=300,                # Giáº£m Ä‘á»™ dÃ i response
    temperature=0.7                # TÄƒng creativity
)

pipeline = MeiRoboPipeline(config)
```

---

## ğŸŒ API ENDPOINTS

### GET /
Root endpoint, tráº£ vá» thÃ´ng tin service

### GET /health
Health check, kiá»ƒm tra server Ä‘ang cháº¡y

**Response:**
```json
{
  "status": "ok",
  "device": "cuda",
  "llm_model": "gpt-4o-mini",
  "tts_voice": "nova"
}
```

### POST /process_audio
Main endpoint: WAV in â†’ WAV out

**Request:**
- `audio`: WAV file (16kHz mono 16-bit recommended)

**Response:**
- WAV file with speech
- Headers:
  - `X-Processing-Time`: Total time (seconds)
  - `X-User-Text`: Transcribed text
  - `X-Reply-Text`: LLM response text

### POST /reset
Reset conversation history

**Response:**
```json
{
  "status": "ok",
  "message": "Conversation history reset"
}
```

### GET /stats
Get system statistics

**Response:**
```json
{
  "config": {
    "device": "cuda",
    "llm_model": "gpt-4o-mini",
    "tts_model": "tts-1",
    "tts_voice": "nova",
    "rag_k": 2
  },
  "conversation_length": 5
}
```

---

## ğŸ“Š PERFORMANCE TIPS

### 1. Giáº£m Latency

```python
# Trong api_server.py hoáº·c core_openai.py
config = MeiRoboConfig(
    rag_k=1,           # Tá»« 2 â†’ 1
    max_tokens=300,    # Tá»« 500 â†’ 300
    tts_speed=1.3      # Tá»« 1.0 â†’ 1.3
)
```

### 2. GPU Acceleration

- Äáº£m báº£o PyTorch detect Ä‘Æ°á»£c CUDA
- Check: `python -c "import torch; print(torch.cuda.is_available())"`
- Náº¿u False â†’ cÃ i `torch` vá»›i CUDA support

### 3. Caching

Implement caching cho repeated queries (tÃ¹y chá»n):

```python
# Trong LLMService
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_chat(self, user_text: str):
    # ...
```

---

## ğŸ› TROUBLESHOOTING

### Lá»—i: "OPENAI_API_KEY not set"
```powershell
$env:OPENAI_API_KEY="sk-your-key"
```

### Lá»—i: "Cannot load FAISS index"
- Check thÆ° má»¥c `vector_stores/faiss_index/` tá»“n táº¡i
- Pháº£i cÃ³ files: `index.faiss`, `index.pkl`

### Lá»—i: Port 5000 Ä‘Ã£ Ä‘Æ°á»£c dÃ¹ng
```powershell
# TÃ¬m process dÃ¹ng port 5000
netstat -ano | findstr :5000

# Kill process (thay PID)
taskkill /PID <PID> /F

# Hoáº·c Ä‘á»•i port trong api_server.py
uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Server cháº­m láº§n Ä‘áº§u
- Láº§n Ä‘áº§u load RAG embeddings model (~400MB)
- Láº§n sau nhanh hÆ¡n (cached)

### FastAPI docs khÃ´ng má»Ÿ
- Check server Ä‘ang cháº¡y: http://localhost:5000/health
- Truy cáº­p thá»§ cÃ´ng: http://localhost:5000/docs

---

## ğŸ”„ MIGRATION FROM OLD CODE

### Náº¿u báº¡n Ä‘Ã£ cÃ³ code cÅ© (Flask version)

**core_openai.py:**
- âœ… ÄÃ£ refactor thÃ nh classes
- âœ… Táº¥t cáº£ functions cÅ© váº«n hoáº¡t Ä‘á»™ng qua pipeline
- âœ… CÃ³ thá»ƒ dÃ¹ng `main()` Ä‘á»ƒ test local

**api_server.py:**
- âœ… Äá»•i tá»« Flask â†’ FastAPI
- âœ… Code ngáº¯n hÆ¡n (~150 vs ~350 lines)
- âœ… Táº¥t cáº£ endpoints tÆ°Æ¡ng thÃ­ch
- âš ï¸ Response format giá»‘ng nhau, nhÆ°ng FastAPI cÃ³ thÃªm auto docs

**robot_client.cpp:**
- âœ… KhÃ´ng thay Ä‘á»•i gÃ¬
- âœ… Váº«n gá»i `/process_audio` nhÆ° cÅ©

---

## ğŸ“ NEXT STEPS

1. **Test local:** `python test_api_local.py`
2. **Setup NGrok:** Get public URL
3. **Update robot code:** Paste NGrok URL vÃ o `robot_client.cpp`
4. **Deploy to Jetson:** Compile vÃ  cháº¡y trÃªn robot

Xem thÃªm: [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) cho hÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§
