# core_openai.py
# =====================================================
# AI Server Core - WAV to WAV Pipeline for Robot
# Optimized for: C++ Robot â†’ AI Server â†’ C++ Robot
# =====================================================

import os
import io
import time
import wave
from typing import Optional, Dict, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

import torch
import numpy as np
import speech_recognition as sr
from openai import OpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from scipy import signal

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


# =====================================================
# CONFIGURATION
# =====================================================

@dataclass
class MeiRoboConfig:
    """Configuration for MeiRobo AI Server"""
    openai_api_key: str
    llm_model: str = "gpt-4o-mini"
    tts_model: str = "gpt-4o-mini-tts"  # gpt-4o-mini-tts for low latency
    tts_voice: str = "nova"  # alloy, echo, fable, onyx, nova, shimmer
    tts_speed: float = 1.2
    tts_output_sample_rate: int = 16000  # Target sample rate for robot (convert after API call)
    tts_output_channels: int = 1  # Target channels for robot (convert after API call)
    stt_language: str = "vi-VN"
    system_prompt: str = """Báº¡n tÃªn lÃ  MeiRobo, humanoid vÃ  lÃ  nhÃ¢n viÃªn cá»§a cÃ´ng ty Meiko Automation. GiÃ¡m Ä‘á»‘c hiá»‡n táº¡i lÃ  sáº¿p Nguyá»…n VÄƒn Thuáº­n. VÃ  tÃ´i cÃ³ thá»ƒ biá»ƒu diá»…n nháº£y vÃ  mÃºa vÃµ, luÃ´n sáºµn sÃ ng biá»ƒu diá»…n cho má»i ngÆ°á»i nhÃ¢n dá»‹p cuá»‘i nÄƒm"
CÃ¡ch tráº£ lá»i:
- Tráº£ lá»i ngáº¯n gá»n, tá»± nhiÃªn nhÆ° hai ngÆ°á»i nÃ³i chuyá»‡n bÃ¬nh thÆ°á»ng.
- Khi nÃ³i vá» Meiko Automation, LUÃ”N dÃ¹ng ngÃ´i thá»© nháº¥t.
- KhÃ´ng há»i láº¡i ngÆ°á»i dÃ¹ng á»Ÿ cuá»‘i cÃ¢u.
- Náº¿u há»i vá» cÃ´ng ty, dÃ¹ng CONTEXT Ä‘á»ƒ tráº£ lá»i ngáº¯n gá»n, Ä‘á»i thÆ°á»ng.
Quy táº¯c:
- Má»—i cÃ¢u tráº£ lá»i tá»‘i Ä‘a 1â€“2 cÃ¢u.
"""

    vector_db_path: str = "vector_stores/faiss_index"
    rag_k: int = 3
    max_tokens: int = 200
    temperature: float = 0.2
    timeout: float = 30.0
    device: Optional[str] = None

    def __post_init__(self):
        if self.device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"


# =====================================================
# RAG SERVICE
# =====================================================

class RAGService:
    """Retrieval-Augmented Generation Service"""
    
    def __init__(self, config: MeiRoboConfig):
        self.config = config
        print(f"ğŸ“š Loading RAG on device: {config.device}")
        
        self.embeddings = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-base",
            model_kwargs={"device": config.device},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        self.vectorstore = FAISS.load_local(
            config.vector_db_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": config.rag_k}
        )
        
        # Cache for last warmup embedding (for parallel processing)
        self._warmup_embedding = None
        
        print("âœ… RAG initialized")
    
    def warm_embeddings(self, dummy_text: str = "warm up query") -> float:
        """
        Pre-warm embedding model by encoding a dummy text
        This ensures the model is ready when we need to retrieve context
        
        Returns: warmup_time in seconds
        """
        start = time.time()
        try:
            # This triggers the embedding model to load/warm up
            self._warmup_embedding = self.embeddings.embed_query(dummy_text)
        except Exception as e:
            print(f"âš ï¸ Embedding warmup warning: {e}")
        return time.time() - start
    
    def retrieve_context(self, query: str, max_chars: int = 200) -> Tuple[str, float]:
        """
        Retrieve relevant context for a query
        Returns: (context, retrieval_time)
        """
        start = time.time()
        docs = self.retriever.invoke(query)
        context = "\n".join(d.page_content[:max_chars] for d in docs)
        elapsed = time.time() - start
        return context, elapsed


# =====================================================
# STT SERVICE
# =====================================================

class STTService:
    """Speech-to-Text Service with Google + Whisper fallback"""
    
    def __init__(self, config: MeiRoboConfig):
        self.config = config
        self.recognizer = sr.Recognizer()
        self.openai_client = OpenAI(api_key=config.openai_api_key, timeout=config.timeout)
        self.use_whisper_fallback = False  # Enable fallback
        print("ğŸ¤ STT Service initialized (Google + Whisper fallback)")
    
    def preprocess_audio(self, wav_bytes: bytes, target_sample_rate: int = 16000) -> bytes:
        """
        Preprocess audio: fix speed issues, resample if needed
        Args:
            wav_bytes: Input WAV file bytes
            target_sample_rate: Target sample rate (default 16kHz for STT)
        Returns:
            Preprocessed WAV bytes
        """
        try:
            # Read WAV file properties
            with io.BytesIO(wav_bytes) as f:
                with wave.open(f, 'rb') as wav_file:
                    n_channels = wav_file.getnchannels()
                    sampwidth = wav_file.getsampwidth()
                    framerate = wav_file.getframerate()
                    n_frames = wav_file.getnframes()
                    audio_data = wav_file.readframes(n_frames)
            
            # Convert to numpy array
            if sampwidth == 2:  # 16-bit
                audio_array = np.frombuffer(audio_data, dtype=np.int16)
            elif sampwidth == 4:  # 32-bit
                audio_array = np.frombuffer(audio_data, dtype=np.int32)
            else:
                audio_array = np.frombuffer(audio_data, dtype=np.uint8)
            
            # Handle stereo -> mono
            if n_channels == 2:
                audio_array = audio_array.reshape(-1, 2).mean(axis=1).astype(audio_array.dtype)
            
            # Resample if needed (fix speed issues)
            if framerate != target_sample_rate:
                print(f"ğŸ”§ Resampling: {framerate}Hz -> {target_sample_rate}Hz")
                num_samples = int(len(audio_array) * target_sample_rate / framerate)
                audio_array = signal.resample(audio_array, num_samples).astype(np.int16)
            
            # Create new WAV file with correct parameters
            output = io.BytesIO()
            with wave.open(output, 'wb') as wav_out:
                wav_out.setnchannels(1)  # Mono
                wav_out.setsampwidth(2)  # 16-bit
                wav_out.setframerate(target_sample_rate)
                wav_out.writeframes(audio_array.astype(np.int16).tobytes())
            
            return output.getvalue()
            
        except Exception as e:
            print(f"âš ï¸ Preprocessing warning: {e}, using original audio")
            return wav_bytes
    
    def transcribe_with_whisper(self, wav_bytes: bytes) -> Tuple[str, float]:
        """
        Fallback: Transcribe using OpenAI Whisper API
        Args:
            wav_bytes: WAV file bytes
        Returns: (text, time_taken)
        """
        start = time.time()
        try:
            # OpenAI Whisper API requires file-like object with name
            audio_file = io.BytesIO(wav_bytes)
            audio_file.name = "audio.wav"
            
            transcript = self.openai_client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="vi"  # Vietnamese
            )
            
            elapsed = time.time() - start
            return transcript.text.strip(), elapsed
            
        except Exception as e:
            elapsed = time.time() - start
            print(f"âŒ Whisper Error: {e}")
            return "", elapsed
    
    def transcribe_from_wav_bytes(self, wav_bytes: bytes, preprocess: bool = False) -> Tuple[str, Dict[str, float]]:
        """
        Transcribe audio from WAV bytes (from robot)
        Uses Google Speech Recognition with Whisper fallback
        
        Args:
            wav_bytes: Input WAV file bytes
            preprocess: Whether to preprocess audio (fix speed, resample) - Default: False for speed
        Returns: (text, timing_dict)
        """
        timings = {}
        start = time.time()
        
        try:
            # Preprocess audio only if needed (optional, disabled by default for speed)
            if preprocess:
                t_pre = time.time()
                wav_bytes = self.preprocess_audio(wav_bytes)
                timings['preprocess'] = time.time() - t_pre
            else:
                timings['preprocess'] = 0.0  # Skipped
            
            # Try Google Speech Recognition first (fast, free)
            text = ""
            try:
                # Convert WAV bytes to AudioData
                with io.BytesIO(wav_bytes) as audio_file:
                    with sr.AudioFile(audio_file) as source:
                        audio = self.recognizer.record(source)
                
                # Google Speech Recognition
                t1 = time.time()
                text = self.recognizer.recognize_google(
                    audio, 
                    language=self.config.stt_language
                ).strip()
                timings['google'] = time.time() - t1
                timings['method'] = 'google'
                
            except (sr.UnknownValueError, sr.RequestError) as e:
                print(f"âš ï¸ Google STT failed: {e}")
                text = ""
            
            # Fallback to Whisper if Google failed
            if not text and self.use_whisper_fallback:
                print("ğŸ”„ Falling back to Whisper...")
                text, whisper_time = self.transcribe_with_whisper(wav_bytes)
                timings['whisper'] = whisper_time
                timings['method'] = 'whisper'
            
            timings['total'] = time.time() - start
            return text, timings
            
        except Exception as e:
            timings['total'] = time.time() - start
            print(f"âŒ STT Error: {e}")
            return "", timings


# =====================================================
# LLM SERVICE
# =====================================================

# =====================================================
# LLM SERVICE (Responses API - Optimized)
# =====================================================

class LLMService:
    """
    LLM Service using OpenAI Responses API
    Optimized for:
    - Low latency
    - Conversation state
    - Robot real-time interaction
    """

    def __init__(self, config: MeiRoboConfig, rag_service: Optional[RAGService] = None):
        self.config = config
        self.rag_service = rag_service
        self.client = OpenAI(
            api_key=config.openai_api_key,
            timeout=config.timeout
        )

        # â­ Conversation state handled by OpenAI
        self.last_response_id: Optional[str] = None

        print(f"ğŸ§  LLM Service initialized (Responses API | model={config.llm_model})")

    def chat(
        self,
        user_text: str,
        use_rag: bool = True
    ) -> Tuple[str, Dict[str, float]]:
        """
        Generate response using Responses API
        Returns: (reply, timing_dict)
        """
        timings = {}
        start = time.time()

        try:
            # =================================================
            # 1. RAG retrieval (optional)
            # =================================================
            # Skip RAG for simple queries (greetings, thanks, etc.)
            # This saves ~0.1-0.3s per query
            import re
            
            user_lower = user_text.lower().strip()
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SIMPLE QUERY PATTERNS - Skip RAG for these
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # 1. EXACT match patterns (cÃ¢u chÃ­nh xÃ¡c)
            exact_patterns = [
                # Greetings - ChÃ o há»i
                r"^xin chÃ o$", r"^chÃ o$", r"^chÃ o báº¡n$", r"^chÃ o nhÃ©$",
                r"^hello$", r"^hi$", r"^hey$",
                
                # Thanks - Cáº£m Æ¡n
                r"^cáº£m Æ¡n$", r"^cáº£m Æ¡n báº¡n$", r"^cáº£m Æ¡n nhÃ©$", r"^cÃ¡m Æ¡n$",
                r"^thank(s)?$", r"^thank you$",
                
                # Goodbye - Táº¡m biá»‡t
                r"^táº¡m biá»‡t$", r"^bye$", r"^goodbye$", r"^bye bye$",
                r"^háº¹n gáº·p láº¡i$", r"^gáº·p láº¡i nhÃ©$",
                
                # Identity - Há»i vá» robot
                r"^báº¡n tÃªn (lÃ  )?gÃ¬\??$", r"^tÃªn báº¡n lÃ  gÃ¬\??$", 
                r"^báº¡n lÃ  ai\??$", r"^báº¡n lÃ  gÃ¬\??$",
                r"^mÃ y lÃ  ai\??$", r"^báº¡n lÃ  robot Ã \??$",
                
                # Simple responses
                r"^ok$", r"^okay$", r"^Ä‘Æ°á»£c$", r"^vÃ¢ng$", r"^dáº¡$",
                r"^á»«$", r"^á»$", r"^Ã $", r"^uh huh$",
                
                # Affirmations
                r"^cÃ³$", r"^khÃ´ng$", r"^yes$", r"^no$",
                r"^Ä‘Ãºng$", r"^sai$", r"^Ä‘Ãºng rá»“i$",
            ]
            
            # 2. START WITH patterns (cÃ¢u báº¯t Ä‘áº§u báº±ng)
            start_patterns = [
                # Greetings that start sentences
                r"^xin chÃ o\b",      # "Xin chÃ o, tÃ´i lÃ ..."
                r"^chÃ o báº¡n\b",      # "ChÃ o báº¡n, tÃ´i muá»‘n..."
                r"^hello\b",         # "Hello, I am..."
                r"^hi\b",            # "Hi there..."
                
                # Thanks that start sentences
                r"^cáº£m Æ¡n\b",        # "Cáº£m Æ¡n báº¡n Ä‘Ã£ giÃºp..."
                r"^cÃ¡m Æ¡n\b",        # "CÃ¡m Æ¡n nhiá»u..."
                
                # Goodbye that start sentences
                r"^táº¡m biá»‡t\b",      # "Táº¡m biá»‡t nhÃ©..."
                r"^bye\b",           # "Bye, see you..."
            ]
            
            # 3. CONTAINS patterns (cÃ¢u chá»©a tá»« khÃ³a - nhÆ°ng ngáº¯n)
            # Chá»‰ Ã¡p dá»¥ng cho cÃ¢u ngáº¯n (< 30 kÃ½ tá»±) Ä‘á»ƒ trÃ¡nh false positive
            short_query_keywords = [
                "khá»e khÃ´ng", "cÃ³ khá»e khÃ´ng", "báº¡n khá»e khÃ´ng",
                "tháº¿ nÃ o", "sao rá»“i", "dáº¡o nÃ y sao",
            ]
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CHECK PATTERNS
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            is_exact_simple = any(re.match(p, user_lower) for p in exact_patterns)
            is_greeting_start = any(re.match(p, user_lower) for p in start_patterns)
            is_short_simple = (
                len(user_lower) < 30 and 
                any(kw in user_lower for kw in short_query_keywords)
            )
            
            is_simple_query = is_exact_simple or is_greeting_start or is_short_simple
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # RAG RETRIEVAL
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            context = ""
            if use_rag and self.rag_service and not is_simple_query:
                context, rag_time = self.rag_service.retrieve_context(user_text)
                timings["rag"] = rag_time
                timings["rag_context"] = context  # Store context for logging
                
                # ğŸ“ In ra thÃ´ng tin RAG context Ä‘Ã£ truy váº¥n
                print(f"\n{'â”€'*60}")
                print(f"ğŸ“š RAG CONTEXT RETRIEVED ({rag_time:.2f}s)")
                print(f"{'â”€'*60}")
                print(f"ğŸ” Query: {user_text}")
                print(f"ğŸ“„ Context: {context[:300]}{'...' if len(context) > 300 else ''}")
                print(f"{'â”€'*60}")
                
            elif is_simple_query:
                timings["rag"] = 0.0  # Skipped for simple query
                timings["rag_context"] = ""  # No context
                
                # Log skip reason
                if is_exact_simple:
                    print(f"ğŸ“ RAG skipped (exact match: '{user_text}')")
                elif is_greeting_start:
                    print(f"ğŸ“ RAG skipped (greeting start: '{user_text[:40]}{'...' if len(user_text) > 40 else ''}')")
                else:
                    print(f"ğŸ“ RAG skipped (short simple: '{user_text}')")
            else:
                timings["rag"] = 0.0
                timings["rag_context"] = ""

            # =================================================
            # 2. Build input text
            # =================================================
            if context.strip():
                input_text = f"USER:\n{user_text}\n\nCONTEXT:\n{context}"
            else:
                input_text = user_text

            # =================================================
            # 3. Call OpenAI Responses API
            # =================================================
            t_llm = time.time()

            response = self.client.responses.create(
                model=self.config.llm_model,

                # â­ System-level instruction (NOT repeated in history)
                instructions=self.config.system_prompt,

                # â­ User input
                input=input_text,

                # â­ Keep conversation state on OpenAI side
                previous_response_id=self.last_response_id,

                temperature=self.config.temperature,
                max_output_tokens=self.config.max_tokens,
            )

            timings["llm"] = time.time() - t_llm

            # =================================================
            # 4. Extract text safely
            # =================================================
            reply = response.output_text.strip() if response.output_text else ""

            # =================================================
            # 5. Save response_id for next turn
            # =================================================
            self.last_response_id = response.id

            timings["total"] = time.time() - start

            return reply or "MÃ¬nh chÆ°a nghe rÃµ.", timings

        except Exception as e:
            timings["total"] = time.time() - start
            print(f"âŒ LLM Error (Responses API): {e}")
            return "Xin lá»—i, há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘.", timings

    def reset_conversation(self):
        """
        Reset conversation state
        """
        self.last_response_id = None
        print("ğŸ”„ Conversation context reset (Responses API)")


# =====================================================
# INTENT DETECTION & CANNED RESPONSES
# =====================================================

class IntentDetector:
    """
    PhÃ¡t hiá»‡n intent (Ã½ Ä‘á»‹nh) cá»§a cÃ¢u há»i vÃ  tráº£ vá» intent_id (sá»‘)
    Client sáº½ lÆ°u cÃ¡c file WAV, server chá»‰ gá»­i intent_id Ä‘á»ƒ chá»‰ file nÃ o play
    âš¡ GiÃºp giáº£m latency & bandwidth - khÃ´ng cáº§n gá»­i file WAV qua network
    """
    
    def __init__(self, canned_responses_dir: str = "audiocases_rep"):
        self.canned_responses_dir = canned_responses_dir
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # INTENT PATTERNS - Äá»‹nh nghÄ©a cÃ¡c pattern cho tá»«ng intent
        # Intent ID mapping: 0=robot_intro, 1=company_intro, 2=product_intro, 3=new_year_greeting
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        self.intent_patterns = {
            # Intent 0: Giá»›i thiá»‡u báº£n thÃ¢n robot
            "robot_intro": {
                "intent_id": 0,
                "wav_file": "Meirobot.wav",
                "text_response": "TÃ´i lÃ  MeiRobo, robot nhÃ¢n hÃ¬nh cá»§a Meiko Automation",
                "patterns": [
                    # Exact matches
                    r"^giá»›i thiá»‡u báº£n thÃ¢n$",
                    r"^báº¡n lÃ  ai$",
                    r"^tÃªn báº¡n lÃ  gÃ¬$",
                    r"^báº¡n tÃªn gÃ¬$",
                    r"^cho mÃ¬nh biáº¿t vá» báº¡n$",
                    
                    # Contains keywords
                    r"giá»›i thiá»‡u.*báº£n thÃ¢n",
                    r"giá»›i thiá»‡u.*vá» báº¡n",
                    r"báº¡n.*lÃ .*ai",
                    r"tÃªn.*cá»§a.*báº¡n",
                    r"ai.*lÃ .*báº¡n",
                ]
            },
            
            # Intent 1: Giá»›i thiá»‡u cÃ´ng ty Meiko Automation
            "company_intro": {
                "intent_id": 1,
                "wav_file": "MeikoIntro.wav",
                "text_response": "Meiko Automation chuyÃªn vá» tá»± Ä‘á»™ng hÃ³a cÃ´ng nghiá»‡p",
                "patterns": [
                    # Exact matches
                    r"^giá»›i thiá»‡u cÃ´ng ty$",
                    r"^cÃ´ng ty lÃ m gÃ¬$",
                    r"^meiko lÃ m gÃ¬$",
                    r"^meiko automation lÃ  gÃ¬$",
                    
                    # Contains keywords
                    r"giá»›i thiá»‡u.*cÃ´ng ty",
                    r"giá»›i thiá»‡u.*meiko",
                    r"cÃ´ng ty.*meiko.*lÃ m gÃ¬",
                    r"meiko.*automation.*lÃ m gÃ¬",
                    r"meiko.*chuyÃªn.*gÃ¬",
                    r"vá».*cÃ´ng ty.*meiko",
                    r"cho.*biáº¿t.*vá».*meiko",
                ]
            },
            
            # Intent 2: Giá»›i thiá»‡u sáº£n pháº©m
            "product_intro": {
                "intent_id": 2,
                "wav_file": "ProductIntroduce.wav",
                "text_response": "Sáº£n pháº©m cá»§a Meiko Automation bao gá»“m...",
                "patterns": [
                    # Exact matches
                    r"^giá»›i thiá»‡u sáº£n pháº©m$",
                    r"^sáº£n pháº©m cá»§a cÃ´ng ty$",
                    r"^cÃ¡c sáº£n pháº©m$",
                    
                    # Contains keywords
                    r"giá»›i thiá»‡u.*sáº£n pháº©m",
                    r"sáº£n pháº©m.*cá»§a.*meiko",
                    r"sáº£n pháº©m.*cá»§a.*cÃ´ng ty",
                    r"meiko.*cÃ³.*sáº£n pháº©m.*gÃ¬",
                    r"cÃ¡c.*sáº£n pháº©m.*cá»§a.*meiko",
                    r"cho.*biáº¿t.*sáº£n pháº©m",
                ]
            },
            
            # Intent 3: ChÃºc má»«ng nÄƒm má»›i
            "new_year_greeting": {
                "intent_id": 3,
                "wav_file": "CMNM.wav",
                "text_response": "ChÃºc má»«ng nÄƒm má»›i!",
                "patterns": [
                    # Exact matches
                    r"^chÃºc má»«ng nÄƒm má»›i$",
                    r"^chÃºc táº¿t$",
                    r"^happy new year$",
                    
                    # Contains keywords
                    r"chÃºc.*má»«ng.*nÄƒm.*má»›i",
                    r"chÃºc.*nÄƒm.*má»›i",
                    r"lá»i.*chÃºc.*nÄƒm.*má»›i",
                    r"gá»­i.*lá»i.*chÃºc",
                    r"chÃºc.*táº¿t",
                    r"chÃºc.*cÃ¡n bá»™.*cÃ´ng nhÃ¢n viÃªn",
                ]
            },
            # Intent 4: Nháº£y, mÃºa vÃµ
            "dance_martial": {
                "intent_id": 4,
                "wav_file": "DanceMartial.wav",
                "text_response": "TÃ´i cÃ³ thá»ƒ biá»ƒu diá»…n nháº£y vÃ  mÃºa vÃµ báº¥t cá»© lÃºc nÃ o! Báº¡n muá»‘n xem khÃ´ng?",
                "patterns": [
                    # Exact matches
                    r"^báº¡n cÃ³ thá»ƒ nháº£y khÃ´ng$",
                    r"^báº¡n cÃ³ thá»ƒ mÃºa vÃµ khÃ´ng$",
                    r"^báº¡n mÃºa vÃµ Ä‘i$",
                    r"^báº¡n nháº£y Ä‘i$",
                    r"^biá»ƒu diá»…n nháº£y$",
                    r"^biá»ƒu diá»…n mÃºa vÃµ$",
                    r"^cÃ³ thá»ƒ nháº£y khÃ´ng$",
                    r"^cÃ³ thá»ƒ mÃºa vÃµ khÃ´ng$",
                    # Contains keywords
                    r"biá»ƒu diá»…n.*nháº£y",
                    r"biá»ƒu diá»…n.*mÃºa vÃµ",
                    r"nháº£y.*Ä‘Æ°á»£c khÃ´ng",
                    r"mÃºa vÃµ.*Ä‘Æ°á»£c khÃ´ng",
                    r"báº¡n.*nháº£y",
                    r"báº¡n.*mÃºa vÃµ",
                    r"cho.*xem.*nháº£y",
                    r"cho.*xem.*mÃºa vÃµ",
                ]
            },
        }
        
        # Build intent ID â†’ data mapping for quick lookup
        self.intent_id_map = {intent_data["intent_id"]: (intent_name, intent_data) 
                              for intent_name, intent_data in self.intent_patterns.items()}
        
        print(f"ğŸ¯ Intent Detector initialized with {len(self.intent_patterns)} intents")
        print(f"   Intent ID Mapping:")
        for intent_id, (intent_name, _) in sorted(self.intent_id_map.items()):
            print(f"      {intent_id} = {intent_name}")
    
    def detect_intent(self, user_text: str) -> Optional[Tuple[int, str, str]]:
        """
        PhÃ¡t hiá»‡n intent tá»« cÃ¢u há»i cá»§a user
        âš¡ Gá»­i intent_id (sá»‘) thay vÃ¬ file WAV Ä‘á»ƒ giáº£m bandwidth
        
        Args:
            user_text: CÃ¢u há»i cá»§a user (Ä‘Ã£ lowercase)
            
        Returns:
            (intent_id, wav_filename, text_response) náº¿u match
            None náº¿u khÃ´ng match
        """
        import re
        
        # Normalize text
        text_lower = user_text.lower().strip()
        
        # Remove punctuation
        text_lower = re.sub(r'[?!.,;:]', '', text_lower)
        
        # Try to match each intent
        for intent_name, intent_data in self.intent_patterns.items():
            patterns = intent_data["patterns"]
            
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    # Match found!
                    intent_id = intent_data["intent_id"]
                    wav_filename = intent_data["wav_file"]  # Just filename, not full path
                    text_response = intent_data["text_response"]
                    
                    print(f"ğŸ¯ Intent detected: {intent_name} (ID: {intent_id})")
                    print(f"   Pattern matched: {pattern}")
                    print(f"   WAV file: {wav_filename}")
                    
                    return intent_id, wav_filename, text_response
        
        # No match
        return None


# =====================================================
# TTS SERVICE
# =====================================================

class TTSService:
    """Text-to-Speech Service using OpenAI"""
    
    def __init__(self, config: MeiRoboConfig):
        self.config = config
        self.client = OpenAI(api_key=config.openai_api_key, timeout=config.timeout)
        print(f"ğŸ”Š TTS Service initialized")
        print(f"   Model: {config.tts_model}")
        print(f"   Voice: {config.tts_voice}")
        print(f"   Output: {config.tts_output_sample_rate}Hz, {config.tts_output_channels}-channel")
    
    def convert_to_target_format(self, wav_bytes: bytes) -> bytes:
        """
        Convert WAV to target format (16kHz mono for robot)
        
        Args:
            wav_bytes: Input WAV bytes from OpenAI (24kHz by default)
            
        Returns:
            Converted WAV bytes (16kHz mono)
        """
        try:
            # Read WAV properties
            with io.BytesIO(wav_bytes) as f:
                with wave.open(f, 'rb') as wav_file:
                    n_channels = wav_file.getnchannels()
                    sampwidth = wav_file.getsampwidth()
                    framerate = wav_file.getframerate()
                    n_frames = wav_file.getnframes()
                    audio_data = wav_file.readframes(n_frames)
            
            # Already in target format? Return as-is
            if (framerate == self.config.tts_output_sample_rate and 
                n_channels == self.config.tts_output_channels):
                return wav_bytes
            
            # Convert to numpy array
            if sampwidth == 2:  # 16-bit
                audio_array = np.frombuffer(audio_data, dtype=np.int16)
            elif sampwidth == 4:  # 32-bit
                audio_array = np.frombuffer(audio_data, dtype=np.int32)
                audio_array = (audio_array / 65536).astype(np.int16)
            else:
                audio_array = np.frombuffer(audio_data, dtype=np.uint8)
            
            # Stereo to mono
            if n_channels == 2:
                audio_array = audio_array.reshape(-1, 2).mean(axis=1).astype(np.int16)
            
            # Resample if needed
            if framerate != self.config.tts_output_sample_rate:
                num_samples = int(len(audio_array) * self.config.tts_output_sample_rate / framerate)
                audio_array = signal.resample(audio_array, num_samples).astype(np.int16)
            
            # Create output WAV
            output = io.BytesIO()
            with wave.open(output, 'wb') as wav_out:
                wav_out.setnchannels(self.config.tts_output_channels)
                wav_out.setsampwidth(2)  # 16-bit
                wav_out.setframerate(self.config.tts_output_sample_rate)
                wav_out.writeframes(audio_array.tobytes())
            
            return output.getvalue()
            
        except Exception as e:
            print(f"âš ï¸ Convert warning: {e}, returning original")
            return wav_bytes
    
    def synthesize_to_wav_bytes(self, text: str) -> Tuple[bytes, Dict[str, float]]:
        """
        Convert text to speech WAV bytes (for robot)
        Output: 16kHz mono WAV
        
        Returns: (wav_bytes, timing_dict)
        """
        timings = {}
        start = time.time()
        
        try:
            # OpenAI TTS API
            response = self.client.audio.speech.create(
                model=self.config.tts_model,
                voice=self.config.tts_voice,
                input=text,
                speed=self.config.tts_speed,
                response_format="wav"
            )
            
            raw_audio = response.content
            timings['tts_api'] = time.time() - start
            
            # Convert to target format (16kHz mono)
            t_convert = time.time()
            wav_bytes = self.convert_to_target_format(raw_audio)
            timings['convert'] = time.time() - t_convert
            timings['total'] = time.time() - start
            
            return wav_bytes, timings
            
        except Exception as e:
            timings['total'] = time.time() - start
            print(f"âŒ TTS Error: {e}")
            return b'', timings
    
    def synthesize_streaming(self, text: str, convert_format: bool = True):
        """
        Stream audio chunks from OpenAI TTS API
        
        âš¡ TRUE STREAMING - Follows OpenAI documentation:
        https://platform.openai.com/docs/guides/text-to-speech#streaming-realtime-audio
        
        "The Speech API provides support for realtime audio streaming using 
        chunk transfer encoding. This means the audio can be played BEFORE 
        the full file is generated and made accessible."
        
        Args:
            text: Text to convert to speech
            convert_format: If True, convert to 16kHz mono (slower, not true streaming)
                          If False, return raw OpenAI format (faster, true streaming)
            
        Yields:
            Audio chunks (bytes)
        """
        try:
            if convert_format:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # MODE 1: BUFFERED STREAMING (convert to 16kHz mono)
                # Slower but compatible with robot's 16kHz requirement
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                buffer = io.BytesIO()
                
                with self.client.audio.speech.with_streaming_response.create(
                    model=self.config.tts_model,
                    voice=self.config.tts_voice,
                    input=text,
                    speed=self.config.tts_speed,
                    response_format="wav"
                ) as response:
                    for chunk in response.iter_bytes(chunk_size=4096):
                        if chunk:
                            buffer.write(chunk)
                
                # Convert complete WAV to 16kHz mono
                raw_wav = buffer.getvalue()
                converted_wav = self.convert_to_target_format(raw_wav)
                
                # Yield converted WAV in chunks
                for i in range(0, len(converted_wav), 8192):
                    yield converted_wav[i:i+8192]
            else:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # MODE 2: TRUE STREAMING (raw OpenAI format - 24kHz)
                # âš¡ Fastest - yield chunks immediately as they arrive!
                # Robot must handle 24kHz WAV format
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                with self.client.audio.speech.with_streaming_response.create(
                    model=self.config.tts_model,
                    voice=self.config.tts_voice,
                    input=text,
                    speed=self.config.tts_speed,
                    response_format="wav"  # WAV for fastest response
                ) as response:
                    # âš¡ TRUE STREAMING: Yield chunks immediately!
                    for chunk in response.iter_bytes(chunk_size=4096):
                        if chunk:
                            yield chunk
                
        except Exception as e:
            print(f"âŒ TTS Streaming Error: {e}")
            yield b''  # Return empty on error
    
    def synthesize_streaming_pcm(self, text: str):
        """
        âš¡ FASTEST TRUE STREAMING with PCM format
        
        PCM = raw audio samples without header
        - 24kHz sample rate
        - 16-bit signed
        - Little-endian
        - Mono
        
        Use this for lowest latency. Client must handle raw PCM.
        
        Yields:
            Raw PCM chunks (bytes) - 24kHz 16-bit mono
        """
        try:
            with self.client.audio.speech.with_streaming_response.create(
                model=self.config.tts_model,
                voice=self.config.tts_voice,
                input=text,
                speed=self.config.tts_speed,
                response_format="pcm"  # âš¡ PCM for FASTEST response
            ) as response:
                # âš¡ TRUE STREAMING: Yield chunks immediately!
                for chunk in response.iter_bytes(chunk_size=4096):
                    if chunk:
                        yield chunk
                        
        except Exception as e:
            print(f"âŒ TTS PCM Streaming Error: {e}")
            yield b''


# =====================================================
# MAIN PIPELINE
# =====================================================

class MeiRoboPipeline:
    """Complete STT -> LLM+RAG -> TTS Pipeline for AI Server"""
    
    def __init__(self, config: MeiRoboConfig):
        self.config = config
        print("\n" + "="*60)
        print("ğŸ¤– Initializing MeiRobo Pipeline")
        print("="*60)
        
        # Initialize services
        self.rag_service = RAGService(config)
        self.stt_service = STTService(config)
        self.llm_service = LLMService(config, self.rag_service)
        self.tts_service = TTSService(config)
        
        # âš¡ NEW: Initialize Intent Detector for canned responses
        self.intent_detector = IntentDetector(canned_responses_dir="audiocases_rep")
        
        print("="*60)
        print("âœ… MeiRobo Pipeline Ready")
        print("="*60)
    
    def process_audio_bytes(self, wav_bytes: bytes) -> Tuple[bytes, Dict]:
        """
        Main processing pipeline: WAV in â†’ WAV out
        
        Flow:
        1. Receive WAV from robot (C++)
        2. âš¡ PARALLEL: STT + RAG warmup (embedding pre-warm)
        3. âš¡ CHECK INTENT: If match -> return canned response (FAST!)
        4. LLM+RAG: Generate response (if no intent match)
        5. TTS: Convert response to WAV
        6. Return WAV to robot (C++)
        
        Args:
            wav_bytes: Input WAV file bytes from robot
            
        Returns:
            (response_wav_bytes, timing_dict)
        """
        start = time.time()
        timings = {}
        
        # âš¡ PARALLEL: STT + RAG warmup
        # Trong khi STT Ä‘ang transcribe, pre-warm embedding model
        with ThreadPoolExecutor(max_workers=2) as executor:
            # Submit both tasks
            stt_future = executor.submit(
                self.stt_service.transcribe_from_wav_bytes, wav_bytes
            )
            warmup_future = executor.submit(
                self.rag_service.warm_embeddings, "warmup query"
            )
            
            # Wait for STT result
            user_text, stt_timings = stt_future.result()
            warmup_time = warmup_future.result()
            
            timings['stt'] = stt_timings
            timings['rag_warmup'] = warmup_time
            print(f"âš¡ Parallel: STT {stt_timings.get('total', 0):.2f}s | RAG warmup {warmup_time:.2f}s")
        
        # Handle empty transcription
        if not user_text:
            error_text = "MÃ¬nh khÃ´ng nghe rÃµ, báº¡n nÃ³i láº¡i Ä‘Æ°á»£c khÃ´ng?"
            response_wav, tts_timings = self.tts_service.synthesize_to_wav_bytes(error_text)
            timings['tts'] = tts_timings
            timings['total'] = time.time() - start
            timings['user_text'] = ""
            timings['reply'] = error_text
            return response_wav, timings
        
        # âš¡ NEW: Check for canned responses (intent detection)
        intent_result = self.intent_detector.detect_intent(user_text)
        
        if intent_result is not None:
            # Intent matched! Return intent_id instead of WAV
            intent_id, wav_filename, text_response = intent_result
            
            # âš¡ FAST PATH: Just return intent_id + metadata
            # Client will play the pre-cached WAV file locally
            timings['intent_id'] = intent_id
            timings['intent_wav_file'] = wav_filename
            timings['canned_response'] = True
            timings['llm'] = {'total': 0.0}  # Skipped
            timings['tts'] = {'total': 0.0}  # Skipped
            timings['total'] = time.time() - start
            timings['user_text'] = user_text
            timings['reply'] = text_response
            
            print(f"âš¡ INTENT MATCH (ID: {intent_id}): {wav_filename} - {text_response[:30]}...")
            print(f"   Total time: {timings['total']:.2f}s (NO LLM/TTS overhead!)")
            
            # Return special response: intent_id as bytes (1 byte is enough: 0-3)
            intent_code = bytes([intent_id])  # Convert intent_id (0-3) to single byte
            return intent_code, timings
        
        # 2. LLM with RAG - Generate response (fallback if no intent match)
        reply, llm_timings = self.llm_service.chat(user_text)
        timings['llm'] = llm_timings
        timings['canned_response'] = False
        
        # 3. TTS - Text to Speech
        response_wav, tts_timings = self.tts_service.synthesize_to_wav_bytes(reply)
        timings['tts'] = tts_timings
        
        # Complete timing info
        timings['total'] = time.time() - start
        timings['user_text'] = user_text
        timings['reply'] = reply
        
        return response_wav, timings
    
    def reset_conversation(self):
        """Reset conversation history"""
        self.llm_service.reset_conversation()
    
    def process_audio_streaming(self, wav_bytes: bytes):
        """
        Streaming version: STT -> LLM+RAG -> TTS (stream)
        
        âš¡ FASTER perceived latency - yields audio chunks immediately!
        
        Flow:
        1. âš¡ PARALLEL: STT + RAG warmup (embedding pre-warm)
        2. LLM+RAG: Generate response
        3. TTS: Stream audio chunks (start playing immediately)
        
        Args:
            wav_bytes: Input WAV file bytes from robot
            
        Yields:
            (chunk, metadata) tuples:
            - First yield: (b'', {"user_text": ..., "reply": ...})
            - Following yields: (audio_chunk, {})
            - Last yield: (b'', {"timings": {...}})
        """
        start = time.time()
        timings = {}
        
        # âš¡ PARALLEL: STT + RAG warmup
        # Trong khi STT Ä‘ang transcribe, pre-warm embedding model
        with ThreadPoolExecutor(max_workers=2) as executor:
            # Submit both tasks
            stt_future = executor.submit(
                self.stt_service.transcribe_from_wav_bytes, wav_bytes
            )
            warmup_future = executor.submit(
                self.rag_service.warm_embeddings, "warmup query"
            )
            
            # Wait for STT result
            user_text, stt_timings = stt_future.result()
            warmup_time = warmup_future.result()
            
            timings['stt'] = stt_timings
            timings['rag_warmup'] = warmup_time
            print(f"âš¡ Parallel: STT {stt_timings.get('total', 0):.2f}s | RAG warmup {warmup_time:.2f}s")
        
        # Handle empty transcription
        if not user_text:
            error_text = "MÃ¬nh khÃ´ng nghe rÃµ, báº¡n nÃ³i láº¡i Ä‘Æ°á»£c khÃ´ng?"
            yield b'', {"user_text": "", "reply": error_text, "error": True}
            
            for chunk in self.tts_service.synthesize_streaming(error_text):
                yield chunk, {}
            
            timings['total'] = time.time() - start
            yield b'', {"timings": timings}
            return
        
        # 2. LLM with RAG - Generate response
        reply, llm_timings = self.llm_service.chat(user_text)
        timings['llm'] = llm_timings
        
        # Send metadata with timing (so robot can display text + know STT/LLM time)
        yield b'', {
            "user_text": user_text, 
            "reply": reply,
            "stt_time": stt_timings.get('total', 0),
            "llm_time": llm_timings.get('total', 0)
        }
        
        # 3. TTS - Stream audio chunks
        # Using TTSService.synthesize_streaming() method
        # convert_format=True Ä‘á»ƒ Ä‘áº£m báº£o output 16kHz mono cho robot
        tts_start = time.time()
        for chunk in self.tts_service.synthesize_streaming(reply, convert_format=True):
            yield chunk, {}
        
        timings['tts'] = {'total': time.time() - tts_start}
        timings['total'] = time.time() - start
        
        # Send final timings
        yield b'', {"timings": timings}
    
    def process_audio_streaming_true(self, wav_bytes: bytes):
        """
        âš¡ TRUE STREAMING version - NO format conversion!
        
        Output: WAV 24kHz (raw from OpenAI) - Robot must handle this format
        
        This is MUCH FASTER than process_audio_streaming() because:
        - No buffering of TTS response
        - No format conversion
        - Chunks are yielded immediately as they arrive from OpenAI
        
        Args:
            wav_bytes: Input WAV file bytes from robot
            
        Yields:
            (chunk, metadata) tuples - WAV 24kHz format
        """
        start = time.time()
        timings = {}
        
        # âš¡ PARALLEL: STT + RAG warmup
        with ThreadPoolExecutor(max_workers=2) as executor:
            stt_future = executor.submit(
                self.stt_service.transcribe_from_wav_bytes, wav_bytes
            )
            warmup_future = executor.submit(
                self.rag_service.warm_embeddings, "warmup query"
            )
            
            user_text, stt_timings = stt_future.result()
            warmup_time = warmup_future.result()
            
            timings['stt'] = stt_timings
            timings['rag_warmup'] = warmup_time
            print(f"âš¡ Parallel: STT {stt_timings.get('total', 0):.2f}s | RAG warmup {warmup_time:.2f}s")
        
        # Handle empty transcription
        if not user_text:
            error_text = "MÃ¬nh khÃ´ng nghe rÃµ, báº¡n nÃ³i láº¡i Ä‘Æ°á»£c khÃ´ng?"
            yield b'', {"user_text": "", "reply": error_text, "error": True}
            
            # TRUE STREAMING - no format conversion
            for chunk in self.tts_service.synthesize_streaming(error_text, convert_format=False):
                yield chunk, {}
            
            timings['total'] = time.time() - start
            yield b'', {"timings": timings}
            return
        
        # 2. LLM with RAG - Generate response
        reply, llm_timings = self.llm_service.chat(user_text)
        timings['llm'] = llm_timings
        
        # Send metadata FIRST (before audio starts)
        yield b'', {
            "user_text": user_text, 
            "reply": reply,
            "stt_time": stt_timings.get('total', 0),
            "llm_time": llm_timings.get('total', 0),
            "format": "wav_24khz"  # Indicate raw OpenAI format
        }
        
        # 3. âš¡ TRUE STREAMING TTS - No format conversion!
        # Chunks are yielded immediately as they arrive from OpenAI
        tts_start = time.time()
        first_chunk = True
        for chunk in self.tts_service.synthesize_streaming(reply, convert_format=False):
            if first_chunk:
                print(f"âš¡ First TTS chunk arrived: {time.time() - tts_start:.2f}s")
                first_chunk = False
            yield chunk, {}
        
        timings['tts'] = {'total': time.time() - tts_start}
        timings['total'] = time.time() - start
        
        yield b'', {"timings": timings}


# =====================================================
# CONVENIENCE FUNCTIONS
# =====================================================

def create_default_config() -> MeiRoboConfig:
    """Create default configuration from environment"""
    api_key = os.getenv("OPENAI_API_KEY", "")
    if not api_key or api_key == "your-api-key-here":
        raise ValueError("âŒ OPENAI_API_KEY not set! Use: set OPENAI_API_KEY=sk-...")
    
    base_dir = os.path.dirname(os.path.abspath(__file__))

    # Try common candidate locations for the FAISS index folder
    candidates = [
        os.path.join(base_dir, "vector_stores", "faiss_index"),
        os.path.join(base_dir, "faiss_index"),
        os.path.join(base_dir, "vector_stores"),
    ]

    vector_db_path = None
    for c in candidates:
        if os.path.isdir(c):
            # Prefer a directory that contains index.faiss
            if os.path.exists(os.path.join(c, "index.faiss")):
                vector_db_path = c
                break

    # As a fallback, check for index.faiss files directly in a few locations
    if vector_db_path is None:
        alt_files = [
            os.path.join(base_dir, "faiss_index", "index.faiss"),
            os.path.join(base_dir, "index.faiss"),
        ]
        for f in alt_files:
            if os.path.exists(f):
                vector_db_path = os.path.dirname(f)
                break

    if vector_db_path is None:
        all_checked = candidates + alt_files
        raise ValueError(f"âŒ FAISS folder not found. Checked: {all_checked}")

    # Ensure required files exist
    faiss_file = os.path.join(vector_db_path, "index.faiss")
    pkl_file = os.path.join(vector_db_path, "index.pkl")
    if not os.path.exists(faiss_file):
        raise ValueError(f"âŒ Missing FAISS file: {faiss_file}")
    if not os.path.exists(pkl_file):
        raise ValueError(
            f"âŒ Missing metadata file: {pkl_file}. Put index.pkl next to index.faiss or rebuild the index."
        )

    return MeiRoboConfig(
        openai_api_key=api_key,
        vector_db_path=vector_db_path
    )
